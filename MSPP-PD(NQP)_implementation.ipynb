{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First MSSP-PD(NQP) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gb\n",
    "from gurobipy import GRB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the synthetic instance of Section 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_number = 25\n",
    "agents_number = 4\n",
    "synthetic_5x5_df = pd.read_csv(\"data/d_it_ij_5x5_1it.csv\",\n",
    "                               header=0,\n",
    "                               names=[\"i\", \"j\", \"d_ij\"])\n",
    "\n",
    "# convert d_ij column into float\n",
    "# print(synthetic_5x5_df.dtypes)\n",
    "synthetic_5x5_df[\"d_ij\"] = synthetic_5x5_df[\"d_ij\"].str.replace(\n",
    "    \",\", \".\").astype(np.float64)\n",
    "# print(synthetic_5x5_df.dtypes)\n",
    "\n",
    "# counting network nodes starting from 0\n",
    "synthetic_5x5_df[[\"i\", \"j\"]] = synthetic_5x5_df[[\"i\", \"j\"]] - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up datas that will be used to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arc = namedtuple(\"Arc\", synthetic_5x5_df.columns)\n",
    "arcs = [Arc(*row) for row in synthetic_5x5_df.itertuples(index=False)]  # as List to avoid nusty bugs\n",
    "\n",
    "nodes = np.unique(np.concatenate((synthetic_5x5_df[\"i\"].to_numpy(),\n",
    "                                  synthetic_5x5_df[\"j\"].to_numpy())))\n",
    "agent_sources = np.array([0, 2, 3, 4])\n",
    "agent_terminus = np.array([20, 22, 23, 24])\n",
    "\n",
    "agents = range(agents_number)\n",
    "# D = synthetic_5x5_df[\"d_ij\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-12-10\n"
     ]
    }
   ],
   "source": [
    "MSPP_PD_NQP_pb = gb.Model(\"First MSPP_PD_NQP_pb\")\n",
    "MSPP_PD_NQP_pb.setParam(\"OutputFlag\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define decision variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var_shape = len(nodes), len(nodes), agents_number\n",
    "\n",
    "R_var_shape = len(nodes), agents_number\n",
    "\n",
    "W_var_shape = len(nodes), agents_number, agents_number\n",
    "\n",
    "\n",
    "X = MSPP_PD_NQP_pb.addMVar(X_var_shape,\n",
    "                           vtype=GRB.BINARY,  # 5) Binary constraints\n",
    "                           name=\"k-th agent traverse arc (i,j)\")\n",
    "\n",
    "R = MSPP_PD_NQP_pb.addMVar(R_var_shape,\n",
    "                           vtype=GRB.BINARY,  # 13) Binary constraints\n",
    "                           name=\"more agents traverse node i\")\n",
    "\n",
    "W = MSPP_PD_NQP_pb.addMVar(W_var_shape,\n",
    "                           vtype=GRB.BINARY,  # 33) Non-negativity constraints\n",
    "                           name=\"agents k and k_ both traverse node i\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3, 28) Objective function (linearized version)\n",
    "distance_objn = gb.quicksum(\n",
    "    arc.d_ij*X[arc.i, arc.j, k]\n",
    "    for arc in arcs for k in agents\n",
    ")\n",
    "penalties_objn = gb.quicksum(\n",
    "    W[i, k, k_] for i in nodes for k in agents for k_ in agents if k_ < k\n",
    ")\n",
    "\n",
    "w_p = 0.5\n",
    "w_d = 1-w_p\n",
    "MSPP_PD_NQP_pb.setObjectiveN(distance_objn, index=0, weight=w_d,\n",
    "                             name=\"Distance\")\n",
    "MSPP_PD_NQP_pb.setObjectiveN(penalties_objn, index=1, weight=w_p,\n",
    "                             name=\"Penalty\")\n",
    "\n",
    "\n",
    "MSPP_PD_NQP_pb.ModelSense = GRB.MINIMIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Flow constraints\n",
    "\n",
    "def compute_flow(X, node, arcs, agent):\n",
    "    flow_out = gb.quicksum(X[node, arc.j, agent]\n",
    "                           for arc in arcs if arc.i == node)\n",
    "    flow_in = gb.quicksum(X[arc.i, node, agent]\n",
    "                          for arc in arcs if arc.j == node)\n",
    "    return flow_out - flow_in\n",
    "\n",
    "\n",
    "for k in agents:\n",
    "    for i in nodes:\n",
    "        if i == agent_sources[k]:\n",
    "            MSPP_PD_NQP_pb.addConstr(compute_flow(X, i, arcs, k) == 1,\n",
    "                                     name=f\"Flow constr related to agent {k} in node {i}\")\n",
    "        elif i == agent_terminus[k]:\n",
    "            MSPP_PD_NQP_pb.addConstr(compute_flow(X, i, arcs, k) == -1,\n",
    "                                     name=f\"Flow constr related to agent {k} in node {i}\")\n",
    "        else:\n",
    "            MSPP_PD_NQP_pb.addConstr(compute_flow(X, i, arcs, k) == 0,\n",
    "                                     name=f\"Flow constr related to agent {k} in node {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,11) Turning on r_i constraints\n",
    "\n",
    "for k in agents:\n",
    "    for node in nodes:\n",
    "        MSPP_PD_NQP_pb.addConstr(\n",
    "            R[node, k] >= gb.quicksum(X[node, arc.j, k]\n",
    "                                      for arc in arcs if arc.i == node)\n",
    "        )\n",
    "        MSPP_PD_NQP_pb.addConstr(\n",
    "            R[node, k] >= gb.quicksum(X[arc.i, node, k]\n",
    "                                      for arc in arcs if arc.j == node)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29) Turning off r_i constraints\n",
    "\n",
    "for k in agents:\n",
    "    for node in nodes:\n",
    "        MSPP_PD_NQP_pb.addConstr(\n",
    "            R[node, k] <= (\n",
    "                gb.quicksum(X[node, arc.j, k] for arc in arcs if arc.i == node) +\n",
    "                gb.quicksum(X[arc.i, node, k] for arc in arcs if arc.j == node)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30-32) Well-defined W variable\n",
    "\n",
    "for node in nodes:\n",
    "    for k in agents:\n",
    "        for k_ in agents:\n",
    "            if k_ < k:\n",
    "                MSPP_PD_NQP_pb.addConstr(\n",
    "                    W[node, k, k_] <= R[node, k]\n",
    "                )\n",
    "                MSPP_PD_NQP_pb.addConstr(\n",
    "                    W[node, k, k_] <= R[node, k_]\n",
    "                )\n",
    "                MSPP_PD_NQP_pb.addConstr(\n",
    "                    W[node, k, k_] >= R[node, k] + R[node, k_] - 1\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSPP_PD_NQP_pb.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the optimization is:\n",
      "optimal\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of the optimization is:\")\n",
    "if MSPP_PD_NQP_pb.Status == 2:\n",
    "    print(\"optimal\")\n",
    "elif MSPP_PD_NQP_pb.Status == 3:\n",
    "    print(\"infeasible\")\n",
    "elif MSPP_PD_NQP_pb.Status == 5:\n",
    "    print(\"unbounded\")\n",
    "else:\n",
    "    print(\"Some other return status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimization founds 4 solutions:\n",
      "Solution 0: Distance=17.0 | Penalty=1.0 | Weighted Total=9.0\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Solution 1: Distance=19.0 | Penalty=3.0 | Weighted Total=11.0\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Solution 2: Distance=24.0 | Penalty=0.0 | Weighted Total=12.0\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Solution 3: Distance=25.0 | Penalty=4.0 | Weighted Total=14.5\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_objectives = MSPP_PD_NQP_pb.NumObj\n",
    "n_solutions = MSPP_PD_NQP_pb.SolCount\n",
    "\n",
    "print(f\"The optimization founds {n_solutions} solutions:\")\n",
    "for sol_n in range(n_solutions):\n",
    "    MSPP_PD_NQP_pb.params.SolutionNumber = sol_n\n",
    "\n",
    "    print(f\"Solution {sol_n}:\", end=\"\")\n",
    "    obj_tot_value = 0\n",
    "    for obj_n in range(n_objectives):\n",
    "        MSPP_PD_NQP_pb.params.ObjNumber = obj_n\n",
    "        obj_tot_value = obj_tot_value + MSPP_PD_NQP_pb.ObjNWeight*MSPP_PD_NQP_pb.ObjNVal\n",
    "        print(f\" {MSPP_PD_NQP_pb.ObjNName}={MSPP_PD_NQP_pb.ObjNVal} \", end=\"|\")\n",
    "    print(f\" Weighted Total={obj_tot_value}\")\n",
    "\n",
    "    for k in agents:\n",
    "        print(f\"Agent {k} will follow the path:\")\n",
    "        for arc in arcs:\n",
    "            if math.isclose(X.X[arc.i, arc.j, k], 1):\n",
    "                print(f\"{arc.i}->{arc.j}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Pareto solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_NQP_distance_and_penalty(X_values, arcs):\n",
    "\n",
    "    def set_r_ik(X, R, arc, agent):\n",
    "        if math.isclose(X[arc.i, arc.j, agent], 1):\n",
    "            R[arc.i, agent] = 1\n",
    "            R[arc.j, agent] = 1\n",
    "\n",
    "    distance_obj = 0\n",
    "    penalty_obj = 0\n",
    "    num_nodes = X_values.shape[0]\n",
    "    num_agents = X_values.shape[-1]\n",
    "\n",
    "    R = np.zeros((num_nodes, num_agents))\n",
    "\n",
    "    for k in range(num_agents):\n",
    "        for arc in arcs:\n",
    "            distance_obj += arc.d_ij*X_values[arc.i, arc.j, k]\n",
    "            set_r_ik(X_values, R, arc, k)\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for k in range(num_agents):\n",
    "            for k_ in range(k):  # k_<k:\n",
    "                penalty_obj += R[i,k]*R[i,k_]\n",
    "\n",
    "    return distance_obj, penalty_obj\n",
    "\n",
    "\n",
    "def path_for_agent(X, k):\n",
    "\n",
    "    return [arc for arc in arcs if math.isclose(X.x[arc.i, arc.j, k], 1)]\n",
    "\n",
    "\n",
    "def get_opt_agents_paths(X):\n",
    "\n",
    "    return {k: path_for_agent(X, k) for k in agents}\n",
    "\n",
    "\n",
    "def print_path(path):\n",
    "\n",
    "    for arc in path:\n",
    "        print(f\"{arc.i}->{arc.j}\", end=\"\\t\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the range of weights to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p_start = 0.01\n",
    "w_p_stop = 1\n",
    "delta = 0.01\n",
    "\n",
    "w_p_range = np.arange(w_p_start, w_p_stop, delta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute solutions to MSPP-PD(NQP) for different weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_distance_values = []\n",
    "opt_penalty_values = []\n",
    "prev_opt_distance, prev_opt_penalty = math.nan, math.nan\n",
    "opt_agents_paths = []\n",
    "# Optimal solution will remain the same for a certain interval of weights of the 2 objectives \n",
    "w_p_intervals_start = []\n",
    "\n",
    "\n",
    "for w_p in w_p_range:\n",
    "\n",
    "    w_d = 1 - w_p\n",
    "\n",
    "    MSPP_PD_NQP_pb.reset()\n",
    "\n",
    "    # Change weights of the 2 objectives\n",
    "    MSPP_PD_NQP_pb.setObjectiveN(\n",
    "        distance_objn, index=0, weight=w_d, name=\"Distance\")\n",
    "    MSPP_PD_NQP_pb.setObjectiveN(\n",
    "        penalties_objn, index=1, weight=w_p, name=\"Penalty\")\n",
    "\n",
    "    MSPP_PD_NQP_pb.optimize()\n",
    "    opt_distance, opt_penalty = evaluate_NQP_distance_and_penalty(X.x, arcs)\n",
    "\n",
    "    if not math.isclose(opt_distance, prev_opt_distance) or not math.isclose(opt_penalty, prev_opt_penalty):\n",
    "\n",
    "        opt_distance_values.append(opt_distance)\n",
    "        opt_penalty_values.append(opt_penalty)\n",
    "        opt_agents_paths.append(get_opt_agents_paths(X))\n",
    "        w_p_intervals_start.append(w_p)\n",
    "\n",
    "        prev_opt_distance, prev_opt_penalty = opt_distance, opt_penalty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the end of the intervals in which solution does not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p_intervals_start.append(w_p_stop)\n",
    "w_p_intervals_stop = [ w_p - delta for w_p in w_p_intervals_start ]\n",
    "del w_p_intervals_start[-1], w_p_intervals_stop[0]\n",
    "w_p_intervals = list(zip(w_p_intervals_start, w_p_intervals_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_p_interval</th>\n",
       "      <th>Optimal distance</th>\n",
       "      <th>Optimal penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.01, 0.2)</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.21000000000000002, 0.54)</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.55, 0.99)</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  w_p_interval  Optimal distance  Optimal penalty\n",
       "0                  (0.01, 0.2)              16.0              5.0\n",
       "1  (0.21000000000000002, 0.54)              17.0              1.0\n",
       "2                 (0.55, 0.99)              18.2              0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pareto_results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"w_p_interval\": w_p_intervals,\n",
    "        \"Optimal distance\": opt_distance_values,\n",
    "        \"Optimal penalty\": opt_penalty_values\n",
    "    }\n",
    ")\n",
    "pareto_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution for w_p in (0.01, 0.2):\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->7\t7->12\t12->18\t18->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Solution for w_p in (0.21000000000000002, 0.54):\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Solution for w_p in (0.55, 0.99):\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->11\t11->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in pareto_results_df.index:\n",
    "    print(f\"Solution for w_p in {w_p_intervals[i]}:\")\n",
    "\n",
    "    for k, path in opt_agents_paths[i].items():\n",
    "        print(f\"Agent {k} will follow the path:\")\n",
    "        print_path(path)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About different solutions\n",
    "\n",
    "We can notice that the solution reported in the article for $w_p\\in[0.01, 0.2]$ (Fig. 5) is diferent from what we found.  \n",
    "However our solution is still an optimal solution since it has the same distance and penalty (therefore same total objective value) of the paper's solution. We'll show this in the following..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set weights as in the first interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p = 0.01\n",
    "w_d = 1 - w_p\n",
    "\n",
    "# Set objectives weights\n",
    "MSPP_PD_NQP_pb.reset()\n",
    "\n",
    "MSPP_PD_NQP_pb.setObjectiveN(\n",
    "    distance_objn, index=0, weight=w_d, name=\"Distance\")\n",
    "MSPP_PD_NQP_pb.setObjectiveN(\n",
    "    penalties_objn, index=1, weight=w_p, name=\"Penalty\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force the solution to be the same as in Fig. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MConstr () *awaiting model update*>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSPP_PD_NQP_pb.addConstr(X[0,6,0] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[6,12,0] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[12,16,0] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[16,20,0] == 1)\n",
    "\n",
    "MSPP_PD_NQP_pb.addConstr(X[2,7,1] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[7,12,1] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[12,16,1] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[16,22,1] == 1)\n",
    "\n",
    "MSPP_PD_NQP_pb.addConstr(X[3,7,2] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[7,12,2] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[12,17,2] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[17,23,2] == 1)\n",
    "\n",
    "MSPP_PD_NQP_pb.addConstr(X[4,8,3] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[8,14,3] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[14,18,3] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[18,24,3] == 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSPP_PD_NQP_pb.optimize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the optimization is:\n",
      "optimal\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of the optimization is:\")\n",
    "if MSPP_PD_NQP_pb.Status == 2:\n",
    "    print(\"optimal\")\n",
    "elif MSPP_PD_NQP_pb.Status == 3:\n",
    "    print(\"infeasible\")\n",
    "elif MSPP_PD_NQP_pb.Status == 5:\n",
    "    print(\"unbounded\")\n",
    "else:\n",
    "    print(\"Some other return status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimization founds 1 solutions:\n",
      "Solution 0: Distance=16.0 | Penalty=5.0 | Weighted Total=15.89\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->16\t16->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->7\t7->12\t12->17\t17->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_objectives = MSPP_PD_NQP_pb.NumObj\n",
    "n_solutions = MSPP_PD_NQP_pb.SolCount\n",
    "\n",
    "print(f\"The optimization founds {n_solutions} solutions:\")\n",
    "for sol_n in range(n_solutions):\n",
    "    MSPP_PD_NQP_pb.params.SolutionNumber = sol_n\n",
    "\n",
    "    print(f\"Solution {sol_n}:\", end=\"\")\n",
    "    obj_tot_value = 0\n",
    "    for obj_n in range(n_objectives):\n",
    "        MSPP_PD_NQP_pb.params.ObjNumber = obj_n\n",
    "        obj_tot_value = obj_tot_value + MSPP_PD_NQP_pb.ObjNWeight*MSPP_PD_NQP_pb.ObjNVal\n",
    "        print(f\" {MSPP_PD_NQP_pb.ObjNName}={MSPP_PD_NQP_pb.ObjNVal} \", end=\"|\")\n",
    "    print(f\" Weighted Total={obj_tot_value}\")\n",
    "\n",
    "    for k in agents:\n",
    "        print(f\"Agent {k} will follow the path:\")\n",
    "        for arc in arcs:\n",
    "            if math.isclose(X.X[arc.i, arc.j, k], 1):\n",
    "                print(f\"{arc.i}->{arc.j}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can state that the solution to the MSPP-PD(NQP) for the considered network is not unique for $w_p\\in[0.01, 0.2]$ since\n",
    "the solution found by us and the one reported in the article have the same objective value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_opt2021",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815aa94ff997c317b35152adf27958c1441995c12fe806fd1ec102e81253708b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
