{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from gurobipy import GRB\n",
    "from utils import file_reader as fr\n",
    "from utils import data_generator as dg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First MSSP-PD(NQP) example\n",
    "\n",
    "In this notebook we'll focus on the MSPP-PD version that penalize node-specific path conflict in relation to the number of potential interactions between agents (i.e. the version that invoke node quadratic penalties (__NQP__)).\n",
    "\n",
    "In the following we'll formulate and solve MSPP-PD(NQP)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "\n",
    "In this section we read the file containing the netwoork instance that we'll use to solve the MSPP.  \n",
    "The instance is the same as the MSPP and MSPP-PD(ABP) cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>it1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     it1\n",
       "0 5  3.0\n",
       "  6  1.0\n",
       "1 5  1.0\n",
       "  6  1.0\n",
       "  7  1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_5x5_df = fr.read_networks_csv(\"data/d_it_ij_5x5_1it.csv\",\n",
    "                                        along=\"cols\")\n",
    "\n",
    "# show some arcs with related weights of the network\n",
    "synthetic_5x5_df.T.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage data\n",
    "\n",
    "In this section we define the variables that we'll use to formulate and solve the MSPP-PD(NQP) problem starting from the dataframe containing the network instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes in the network\n",
    "nodes = dg.get_nodes(synthetic_5x5_df)\n",
    "\n",
    "\n",
    "# weighted arcs in the network\n",
    "# the weight of each arc represents the ditance between 2 nodes\n",
    "w_arcs = [dg.WArc(i, j, synthetic_5x5_df.loc[\"it1\", (i, j)], idx)\n",
    "          for idx, (i, j) in enumerate(synthetic_5x5_df.columns)]\n",
    "\n",
    "\n",
    "# agets that has to be routed\n",
    "agents_sources = [0, 2, 3, 4]\n",
    "agents_terminus = [20, 22, 23, 24]\n",
    "agents_idxs = [0, 1, 2, 3]\n",
    "agents = [dg.Agent(s, t, idx) for s, t, idx in zip(\n",
    "    agents_sources, agents_terminus, agents_idxs)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation and solution\n",
    "\n",
    "In this section we formulate and solve the MSPP-PD(NQP) problem using Gurobi.  \n",
    "The formulations follows the one reported in section 2.5 of the paper\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-12-10\n"
     ]
    }
   ],
   "source": [
    "MSPP_PD_NQP_pb = gb.Model(\"First MSPP_PD_NQP_pb\")\n",
    "# MSPP_PD_NQP_pb.setParam(\"OutputFlag\", 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define decision variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_var_shape = len(w_arcs), len(agents)\n",
    "R_var_shape = len(nodes), len(agents)\n",
    "W_var_shape = len(nodes), len(agents), len(agents)\n",
    "\n",
    "\n",
    "X = MSPP_PD_NQP_pb.addMVar(X_var_shape,\n",
    "                           vtype=GRB.BINARY,  # 5) Binary constraints\n",
    "                           name=\"X\")\n",
    "\n",
    "R = MSPP_PD_NQP_pb.addMVar(R_var_shape,\n",
    "                           vtype=GRB.BINARY,  # 13) Binary constraints\n",
    "                           name=\"R\")\n",
    "\n",
    "W = MSPP_PD_NQP_pb.addMVar(W_var_shape,\n",
    "                           vtype=GRB.BINARY,  # 33) Non-negativity constraints\n",
    "                           name=\"W\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the 2 objective functions (distance and penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for the 2 objectives\n",
    "w_p = 0.5\n",
    "w_d = 1-w_p\n",
    "\n",
    "\n",
    "# 1:3, 28) Objective functions\n",
    "distance_obj = gb.quicksum(\n",
    "    arc.w * X[arc.idx, agent.idx]\n",
    "    for arc in w_arcs for agent in agents\n",
    ")\n",
    "penalty_obj = gb.quicksum(  # linearized version\n",
    "    W[node, agent.idx, agent_.idx] for node in nodes\n",
    "    for agent in agents for agent_ in agents if agent_.idx < agent.idx\n",
    ")\n",
    "\n",
    "\n",
    "# as result objective will be: f(X, Psi) = w_d**distance_obj(X) + w_p*penalty_obj(W)\n",
    "MSPP_PD_NQP_pb.setObjectiveN(distance_obj, index=0, weight=w_d,\n",
    "                             name=\"Distance\")\n",
    "MSPP_PD_NQP_pb.setObjectiveN(penalty_obj, index=1, weight=w_p,\n",
    "                             name=\"Penalty\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Flow constraints\n",
    "\n",
    "def compute_flow(X, node, w_arcs, agent):\n",
    "    \"\"\"Compute the flow in a node of a network for a given agent that traverse it\n",
    "\n",
    "    Args:\n",
    "        X (gb.MVar): X decision variables of a MSPP or MSPP-PD problem\n",
    "        node (int): the node on which compute the flow\n",
    "        w_arcs (list): list of WArc that compose the network\n",
    "        agent (Agent): the agent for which to calculate the flow\n",
    "\n",
    "    Returns:\n",
    "        gb.MLinExpr: linear matrix expression that represent the flow for the agent in the node\n",
    "    \"\"\"\n",
    "\n",
    "    flow_out = gb.quicksum(\n",
    "        X[arc.idx, agent.idx]\n",
    "        for arc in w_arcs if arc.i == node\n",
    "    )\n",
    "    flow_in = gb.quicksum(\n",
    "        X[arc.idx, agent.idx]\n",
    "        for arc in w_arcs if arc.j == node\n",
    "    )\n",
    "    return flow_out - flow_in\n",
    "\n",
    "\n",
    "for agent in agents:\n",
    "    for node in nodes:\n",
    "        if node == agent.source:\n",
    "            MSPP_PD_NQP_pb.addConstr(compute_flow(X, node, w_arcs, agent) == 1)\n",
    "        elif node == agent.terminus:\n",
    "            MSPP_PD_NQP_pb.addConstr(\n",
    "                compute_flow(X, node, w_arcs, agent) == -1)\n",
    "        else:\n",
    "            MSPP_PD_NQP_pb.addConstr(compute_flow(X, node, w_arcs, agent) == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,11) Turning on r_i constraints\n",
    "for arc in w_arcs:\n",
    "    for agent in agents:\n",
    "        MSPP_PD_NQP_pb.addConstr(\n",
    "            R[arc.i, agent.idx] >= X[arc.idx, agent.idx]\n",
    "        )\n",
    "        MSPP_PD_NQP_pb.addConstr(\n",
    "            R[arc.j, agent.idx] >= X[arc.idx, agent.idx]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29) Turning off r_i constraints\n",
    "for node in nodes:\n",
    "    for agent in agents:\n",
    "        MSPP_PD_NQP_pb.addConstr(\n",
    "            R[node, agent.idx] <= (\n",
    "                gb.quicksum(X[arc.idx, agent.idx] for arc in w_arcs if arc.i == node) +\n",
    "                gb.quicksum(X[arc.idx, agent.idx]\n",
    "                            for arc in w_arcs if arc.j == node)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30-32) Well-defined W variable\n",
    "for node in nodes:\n",
    "    for agent in agents:\n",
    "        for agent_ in agents:\n",
    "            if agent_.idx < agent.idx:\n",
    "                MSPP_PD_NQP_pb.addConstr(\n",
    "                    W[node, agent.idx, agent_.idx] <= R[node, agent.idx]\n",
    "                )\n",
    "                MSPP_PD_NQP_pb.addConstr(\n",
    "                    W[node, agent.idx, agent_.idx] <= R[node, agent_.idx]\n",
    "                )\n",
    "                MSPP_PD_NQP_pb.addConstr(\n",
    "                    W[node, agent.idx, agent_.idx] >=\n",
    "                    R[node, agent.idx] + R[node, agent_.idx] - 1\n",
    "                )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.0 build v10.0.0rc2 (linux64)\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 2 physical cores, 2 logical processors, using up to 2 threads\n",
      "\n",
      "Optimize a model with 1066 rows, 708 columns and 2814 nonzeros\n",
      "Model fingerprint: 0x6cf369bf\n",
      "Variable types: 0 continuous, 708 integer (708 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 3e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Multi-objectives: starting optimization with 2 objectives (1 combined) ...\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Multi-objectives: optimize objective 1 (weighted) ...\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Optimize a model with 1066 rows, 708 columns and 2814 nonzeros\n",
      "Model fingerprint: 0x631136e3\n",
      "Variable types: 0 continuous, 708 integer (708 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [5e-01, 2e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Presolve removed 954 rows and 611 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 112 rows, 97 columns, 356 nonzeros\n",
      "Variable types: 0 continuous, 97 integer (97 binary)\n",
      "Found heuristic solution: objective 11.0000000\n",
      "\n",
      "Root relaxation: objective 9.000000e+00, 43 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       9.0000000    9.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (43 simplex iterations) in 0.20 seconds (0.00 work units)\n",
      "Thread count was 2 (of 2 available processors)\n",
      "\n",
      "Solution count 2: 9 11 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 9.000000000000e+00, best bound 9.000000000000e+00, gap 0.0000%\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "Multi-objectives: solved in 0.22 seconds (0.00 work units), solution count 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MSPP_PD_NQP_pb.optimize()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report results\n",
    "\n",
    "In this section we report the results that we've obtained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the optimization is:\n",
      "optimal\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of the optimization is:\")\n",
    "if MSPP_PD_NQP_pb.Status == 2:\n",
    "    print(\"optimal\")\n",
    "elif MSPP_PD_NQP_pb.Status == 3:\n",
    "    print(\"infeasible\")\n",
    "elif MSPP_PD_NQP_pb.Status == 5:\n",
    "    print(\"unbounded\")\n",
    "else:\n",
    "    print(\"Some other return status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimization founds 2 solutions:\n",
      "\n",
      "Solution 0: Distance=17.0 | Penalty=1.0 | Weighted Total=9.0\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Solution 1: Distance=19.0 | Penalty=3.0 | Weighted Total=11.0\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_objectives = MSPP_PD_NQP_pb.NumObj\n",
    "n_solutions = MSPP_PD_NQP_pb.SolCount\n",
    "\n",
    "print(f\"The optimization founds {n_solutions} solutions:\")\n",
    "print()\n",
    "for sol_n in range(n_solutions):\n",
    "    MSPP_PD_NQP_pb.params.SolutionNumber = sol_n\n",
    "\n",
    "    print(f\"Solution {sol_n}:\", end=\"\")\n",
    "    obj_tot_value = 0\n",
    "    for obj_n in range(n_objectives):\n",
    "        MSPP_PD_NQP_pb.params.ObjNumber = obj_n\n",
    "        obj_tot_value = obj_tot_value + MSPP_PD_NQP_pb.ObjNWeight*MSPP_PD_NQP_pb.ObjNVal\n",
    "        print(f\" {MSPP_PD_NQP_pb.ObjNName}={MSPP_PD_NQP_pb.ObjNVal} \", end=\"|\")\n",
    "    print(f\" Weighted Total={obj_tot_value}\")\n",
    "\n",
    "    for agent in agents:\n",
    "        print(f\"Agent {agent.idx} will follow the path:\")\n",
    "        for arc in w_arcs:\n",
    "            if math.isclose(X.x[arc.idx, agent.idx], 1):\n",
    "                print(f\"{arc.i}->{arc.j}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Pareto solutions\n",
    "\n",
    "In this section we analize a scenario on which the decision-maker is interested in minimizing both distance and penalty objective function, but their relative priorities are not known a priori.  \n",
    "In this case is of interest to identify the non-dominated solutions on the Pareto frontier such that the decision-maker can select its priorities a posteriori via the identification of a preferred solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pb_objectives(problem):\n",
    "    \"\"\"Get optimal objectives from an optimization problem\n",
    "\n",
    "    Args:\n",
    "        problem (gb.Model): The optimization problem\n",
    "\n",
    "    Returns:\n",
    "        list: a list with the optimal values of the different problem's objectives\n",
    "    \"\"\"\n",
    "\n",
    "    assert problem.Status == GRB.Status.OPTIMAL\n",
    "\n",
    "    problem.params.SolutionNumber = 0  # Set best solution found\n",
    "    opt_solution = []\n",
    "\n",
    "    # Add to opt_solution the value of each objective\n",
    "    for obj in range(problem.NumObj):\n",
    "        problem.params.ObjNumber = obj\n",
    "        opt_solution.append(problem.ObjNVal)\n",
    "\n",
    "    return opt_solution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the range of weights to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p_start, w_p_stop, delta = 0.01, 1, 0.01\n",
    "\n",
    "w_p_range = np.arange(w_p_start, w_p_stop, delta)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute solutions to MSPP-PD(NQP) for different weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSPP_PD_NQP_pb.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "opt_distance_values, opt_penalty_values = [], []\n",
    "prev_opt_distance, prev_opt_penalty = math.nan, math.nan  # initialization values\n",
    "\n",
    "opt_solutions = []\n",
    "\n",
    "# Optimal solutions will remain the same for a certain interval of weights of the 2 objectives\n",
    "w_p_intervals_start, w_p_intervals_stop = [], []\n",
    "\n",
    "\n",
    "# actual computations\n",
    "for w_p in w_p_range:\n",
    "\n",
    "    w_d = 1 - w_p\n",
    "\n",
    "    # Discard previous solutions\n",
    "    MSPP_PD_NQP_pb.reset()\n",
    "\n",
    "    # Change weights of the 2 objectives\n",
    "    MSPP_PD_NQP_pb.setObjectiveN(\n",
    "        distance_obj, index=0, weight=w_d, name=\"Distance\")\n",
    "    MSPP_PD_NQP_pb.setObjectiveN(\n",
    "        penalty_obj, index=1, weight=w_p, name=\"Penalty\")\n",
    "\n",
    "    # Optimizing and retrieving new solutions for the new set of weights\n",
    "    MSPP_PD_NQP_pb.optimize()\n",
    "    opt_distance, opt_penalty = evaluate_pb_objectives(MSPP_PD_NQP_pb)\n",
    "\n",
    "    # Check if changing the weights change the opt solution\n",
    "    if not math.isclose(opt_distance, prev_opt_distance) or not math.isclose(opt_penalty, prev_opt_penalty):  # true if nan\n",
    "\n",
    "        opt_distance_values.append(opt_distance)\n",
    "        opt_penalty_values.append(opt_penalty)\n",
    "        opt_solutions.append(X.x)\n",
    "        w_p_intervals_start.append(w_p)\n",
    "\n",
    "        prev_opt_distance, prev_opt_penalty = opt_distance, opt_penalty\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the end of intervals wherein solution does not change and the intervals themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.01, 0.2), (0.21000000000000002, 0.54), (0.55, 0.99)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_p_intervals_start.append(w_p_stop)\n",
    "w_p_intervals_stop = [w_p - delta for w_p in w_p_intervals_start]\n",
    "del w_p_intervals_start[-1], w_p_intervals_stop[0]\n",
    "w_p_intervals = list(zip(w_p_intervals_start, w_p_intervals_stop))\n",
    "w_p_intervals\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective functions at the optimum for different weights intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_p interval</th>\n",
       "      <th>Optimal distance</th>\n",
       "      <th>Optimal penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.01, 0.2)</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.21000000000000002, 0.54)</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.55, 0.99)</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  w_p interval  Optimal distance  Optimal penalty\n",
       "0                  (0.01, 0.2)              16.0              5.0\n",
       "1  (0.21000000000000002, 0.54)              17.0              1.0\n",
       "2                 (0.55, 0.99)              18.2              0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pareto_results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"w_p interval\": w_p_intervals,\n",
    "        \"Optimal distance\": opt_distance_values,\n",
    "        \"Optimal penalty\": opt_penalty_values\n",
    "    }\n",
    ")\n",
    "pareto_results_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal agents' paths for different weights intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent optimal paths for w_p in (0.01, 0.2):\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->18\t18->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->7\t7->12\t12->17\t17->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Agent optimal paths for w_p in (0.21000000000000002, 0.54):\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n",
      "Agent optimal paths for w_p in (0.55, 0.99):\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->11\t11->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->17\t17->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->9\t9->13\t13->19\t19->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x_opt, w_p_interval in zip(opt_solutions, w_p_intervals):\n",
    "    print(f\"Agent optimal paths for w_p in {w_p_interval}:\")\n",
    "\n",
    "    for agent in agents:\n",
    "        print(f\"Agent {agent.idx} will follow the path:\")\n",
    "        for arc in w_arcs:\n",
    "            if math.isclose(x_opt[arc.idx, agent.idx], 1):\n",
    "                print(f\"{arc.i}->{arc.j}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About different solutions\n",
    "\n",
    "We can notice that the solution reported in the article for $w_p\\in[0.01, 0.2]$ (Fig. 5) is diferent from what we found.  \n",
    "However our solution is still an optimal solution since it has the same total distance and penalty (therefore same total objective value) of the paper's solution. We'll show this in the following..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful function to retrieve the index of an arc from its starting and ending nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_ij_to_idx(w_arcs_list, i, j):\n",
    "    \"\"\"Gets the arc index of the WArc which has i as starting node and j as ending node\n",
    "\n",
    "    Args:\n",
    "        w_arcs_list (list): List of WArcs where to look for the desired arc index\n",
    "        i (int): starting node of the desired arc\n",
    "        j (int): ending node of the desired arc\n",
    "\n",
    "    Returns:\n",
    "        int: the index of the desired WArc or None if the desired arc is not present in w_arcs_list\n",
    "    \"\"\"\n",
    "\n",
    "    return next((arc.idx for arc in w_arcs_list if arc.i == i and arc.j == j), None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set weights as in the first interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_p = 0.01\n",
    "w_d = 1 - w_p\n",
    "\n",
    "\n",
    "MSPP_PD_NQP_pb.setObjectiveN(\n",
    "    distance_obj, index=0, weight=w_d, name=\"Distance\")\n",
    "MSPP_PD_NQP_pb.setObjectiveN(\n",
    "    penalty_obj, index=1, weight=w_p, name=\"Penalty\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force the solution to be the same as in Fig. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MConstr () *awaiting model update*>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agent 0 path\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 0, 6), 0] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 6, 12), 0] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 12, 16), 0] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 16, 20), 0] == 1)\n",
    "\n",
    "# Agent 1 path\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 2, 7), 1] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 7, 12), 1] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 12, 16), 1] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 16, 22), 1] == 1)\n",
    "\n",
    "# Agent 2 path\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 3, 7), 2] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 7, 12), 2] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 12, 17), 2] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 17, 23), 2] == 1)\n",
    "\n",
    "# Agent 3 path\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 4, 8), 3] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 8, 14), 3] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 14, 18), 3] == 1)\n",
    "MSPP_PD_NQP_pb.addConstr(X[arc_ij_to_idx(w_arcs, 18, 24), 3] == 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard previous solutions\n",
    "MSPP_PD_NQP_pb.reset()\n",
    "\n",
    "MSPP_PD_NQP_pb.optimize()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of the optimization is:\n",
      "optimal\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of the optimization is:\")\n",
    "if MSPP_PD_NQP_pb.Status == 2:\n",
    "    print(\"optimal\")\n",
    "elif MSPP_PD_NQP_pb.Status == 3:\n",
    "    print(\"infeasible\")\n",
    "elif MSPP_PD_NQP_pb.Status == 5:\n",
    "    print(\"unbounded\")\n",
    "else:\n",
    "    print(\"Some other return status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimization founds 1 solutions:\n",
      "\n",
      "Solution 0: Distance=16.0 | Penalty=5.0 | Weighted Total=15.89\n",
      "Agent 0 will follow the path:\n",
      "0->6\t6->12\t12->16\t16->20\t\n",
      "Agent 1 will follow the path:\n",
      "2->7\t7->12\t12->16\t16->22\t\n",
      "Agent 2 will follow the path:\n",
      "3->7\t7->12\t12->17\t17->23\t\n",
      "Agent 3 will follow the path:\n",
      "4->8\t8->14\t14->18\t18->24\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_objectives = MSPP_PD_NQP_pb.NumObj\n",
    "n_solutions = MSPP_PD_NQP_pb.SolCount\n",
    "\n",
    "print(f\"The optimization founds {n_solutions} solutions:\")\n",
    "print()\n",
    "for sol_n in range(n_solutions):\n",
    "    MSPP_PD_NQP_pb.params.SolutionNumber = sol_n\n",
    "\n",
    "    print(f\"Solution {sol_n}:\", end=\"\")\n",
    "    obj_tot_value = 0\n",
    "    for obj_n in range(n_objectives):\n",
    "        MSPP_PD_NQP_pb.params.ObjNumber = obj_n\n",
    "        obj_tot_value = obj_tot_value + MSPP_PD_NQP_pb.ObjNWeight*MSPP_PD_NQP_pb.ObjNVal\n",
    "        print(f\" {MSPP_PD_NQP_pb.ObjNName}={MSPP_PD_NQP_pb.ObjNVal} \", end=\"|\")\n",
    "    print(f\" Weighted Total={obj_tot_value}\")\n",
    "\n",
    "    for agent in agents:\n",
    "        print(f\"Agent {agent.idx} will follow the path:\")\n",
    "        for arc in w_arcs:\n",
    "            if math.isclose(X.x[arc.idx, agent.idx], 1):\n",
    "                print(f\"{arc.i}->{arc.j}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can state that the solution to the MSPP-PD(NQP) for the considered network is not unique for $w_p\\in[0.01, 0.2]$ since\n",
    "the solution found by us and the one reported in the paper have the same objective value\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_opt2021",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "815aa94ff997c317b35152adf27958c1441995c12fe806fd1ec102e81253708b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
